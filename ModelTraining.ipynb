{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "import argparse\n",
    "from scipy.sparse import vstack\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sparselearning.core import add_sparse_args, CosineDecay, Masking\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean()#.asscalar()\n",
    "\n",
    "def minmaxscaler(data,test):\n",
    "    min = np.amin(data)\n",
    "    max = np.amax(data)    \n",
    "    return (data - min)/(max-min),(test-min)/(max-min)\n",
    "\n",
    "def feature_normalize(data,test):\n",
    "    mu = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return (data - mu)/std, (test-mu)/std\n",
    "\n",
    "def to_number(x):\n",
    "    if x=='BaseLine':\n",
    "        return [0]\n",
    "    else:\n",
    "        lb = []\n",
    "        for i in x:\n",
    "            lb.append(int(i))\n",
    "        return lb\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv1d') != -1:\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        #init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('BatchNorm1d') != -1:\n",
    "        init.constant_(m.weight.data, 1)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "import random\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = features.shape[0]\n",
    "    indices = list(range(num_examples))\n",
    "    random.seed(epoch)\n",
    "    random.shuffle(indices) \n",
    "    for i in range(0, num_examples/batch_size*batch_size, batch_size):\n",
    "        j = indices[i: min(i + batch_size, num_examples)]\n",
    "        yield (torch.FloatTensor(features[j]), torch.LongTensor(labels[j]))  \n",
    "\n",
    "def replace_layers(model, i, indexes, layers):\n",
    "    if i in indexes:\n",
    "        return layers[indexes.index(i)]\n",
    "    return model[i]\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    #lr = LR * (0.3 ** (epoch // 20))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/mnt/sda1/jianwen/sidescan/\"\n",
    "#data/  model/  net_50.pkl\n",
    "#data/multiple_mx.pkl  model/trainedmodel.pkl\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_planes,out_planes,kernel_size=8,stride=1):                          \n",
    "        \"3x3 convolution with padding\"\n",
    "        return nn.Conv1d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride=stride,\n",
    "                padding=(kernel_size-1)/2,\n",
    "                bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "        def __init__(self,in_planes,planes,kernel_size,stride=1,downsample=None):\n",
    "                super(BasicBlock, self).__init__()\n",
    "                self.conv1 = conv(in_planes,planes,kernel_size,1)\n",
    "                self.bn1 = nn.BatchNorm1d(planes)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.downsample = downsample\n",
    "                self.stride = 1 \n",
    "                #\n",
    "                self.conv2 = conv(planes,planes,kernel_size,1)\n",
    "                self.bn2 = nn.BatchNorm1d(planes)\n",
    "                #\n",
    "                self.conv3 = conv(planes,planes,kernel_size,1)\n",
    "                self.bn3 = nn.BatchNorm1d(planes)\n",
    "        def forward(self,x):\n",
    "                residual = x\n",
    "                out = self.conv1(x)\n",
    "                out = self.bn1(out)\n",
    "                out = self.relu(out)\n",
    "                #\n",
    "                out = self.conv2(out)\n",
    "                out = self.bn2(out)\n",
    "                out = self.relu(out)\n",
    "                #                                                                                    \n",
    "                out = self.conv3(out)\n",
    "                out = self.bn3(out)\n",
    "        \n",
    "                if self.downsample is not None:\n",
    "                    residual = self.downsample(x)  \n",
    "                out += residual\n",
    "                out = self.relu(out)\n",
    "                return out \n",
    "    \n",
    "class ResEncoder(nn.Module):\n",
    "        def __init__(self,block,kernel_size,num_classes=6,in_planes=10):#block means BasicBlock\n",
    "            self.in_planes = in_planes\n",
    "            super(ResEncoder,self).__init__()\n",
    "            self.layer1 = self._make_layer(block,kernel_size[0],64)#128)\n",
    "            self.layer2 = self._make_layer(block,kernel_size[1],128)#256)\n",
    "            self.layer3 = self._make_layer(block,kernel_size[2],128)#512)\n",
    "            \n",
    "            self.pool = nn.MaxPool1d(2,stride=2)\n",
    "            #self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.fc = nn.Linear(64,num_classes)\n",
    "            self.softmax = torch.nn.Softmax(dim=1)\n",
    "            #self.inm = nn.InstanceNorm1d(256)\n",
    " \n",
    "        def _make_layer(self, block, kernel_size, planes, stride=1):\n",
    "            downsample = None\n",
    "            if stride != 1 or self.in_planes != planes:\n",
    "                downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes,\n",
    "                kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes),\n",
    "              )\n",
    "            layers = []\n",
    "            layers.append(block(self.in_planes,planes,kernel_size,stride,downsample))\n",
    "            self.in_planes = planes\n",
    "            #for i in range(1,blocks):\n",
    "            #       layers.append(block(self.inplanes,planes))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self,x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.layer3(x) #batch,512,60 \n",
    "            x = x[:,x.size(1)/2:,:].mul(self.softmax(x[:,:x.size(1)/2,:])) #batch,256,60\n",
    "            x = x.sum(2)\n",
    "            x = x.view(x.size(0),-1)\n",
    "            #x = self.inm(x)\n",
    "            x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,label = pickle.load(open(BASE+'data/multiple_mx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16010, 60, 17), (16010,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape,label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train,x_test,y_train,y_test,path):\n",
    "    epochs = 1500\n",
    "    batch_size = x_train.shape/10 #you may need to adjust the number\n",
    "    # Training\n",
    "    print x_train.shape\n",
    "    #define network\n",
    "    net = ResEncoder(BasicBlock,[9,5,3],7,17)\n",
    "    net = nn.DataParallel(net)\n",
    "    net = net.cuda()\n",
    "    net.apply(weights_init) \n",
    "    LR = 0.01\n",
    "    loss_func = torch.nn.CrossEntropyLoss()#loss function\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999))\n",
    "    net.train()#change to train mode(activate the dropout)\n",
    "    #train          \n",
    "    for epoch in range(epochs):  \n",
    "        sum_loss = 0\n",
    "        for step,(batch_x,batch_y) in enumerate(data_iter(batch_size,x_train,y)):\n",
    "            batch_x = Variable(batch_x).cuda()\n",
    "            batch_y = Variable(batch_y).cuda()\n",
    "            outputs = net(batch_x)\n",
    "            loss = loss_func(outputs,batch_y)    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()                     # calculate the gradients\n",
    "            sum_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            gc.collect()\n",
    "            if (step+1) % sp == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, (step+1)*batch_size, x_train.shape[0],\n",
    "                        100*batch_size * (step+1) / x_train.shape[0], sum_loss/sp))\n",
    "                sum_loss = 0\n",
    "                net.eval()\n",
    "                y_pred = F.softmax(net(x_t)).cpu()\n",
    "                macro = f1_score(y_test,y_pred.detach().numpy().argmax(axis=1),labels=[0,1,2,3,4,5,6],average='macro')\n",
    "                acc = accuracy(y_pred.detach().numpy(),y_test)\n",
    "                net.train()\n",
    "                if macro>best_accuracy:\n",
    "                    best_accuracy = macro\n",
    "                    beat_epoch = epoch\n",
    "                    torch.save(net,path)\n",
    "                print 'Train Epoch: {}, Test Accuracy:{}, Macro F1:{}'.format(epoch,acc,macro) \n",
    "        print best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Start..'\n",
    "dataset,label = pickle.load(open(BASE+'data/multiple_mx.pkl','rb'))\n",
    "dataset = dataset.swapaxes(1,2).astype('float')#(X,17,60)\n",
    "#filter\n",
    "print dataset.shape\n",
    "for i in range(7):#print the number for different classes\n",
    "    print label[label==i].shape\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "for n, (train, test) in enumerate(skf.split(dataset,label)):#cross validation\n",
    "    best_accuracy = 0\n",
    "    print train, test\n",
    "    print np.array(dataset)[train].shape, np.array(dataset)[test].shape\n",
    "    x_train = np.array(dataset)[train]\n",
    "    x_t = np.array(dataset)[test]\n",
    "    x_t = torch.FloatTensor(np.array(dataset)[test]).cuda()\n",
    "    y = label[train]\n",
    "    y_test = label[test]\n",
    "    print 'train:',y.shape,'test:',y_test.shape\n",
    "    path = BASE+'model/testmodel.pkl'\n",
    "    #train the model and save it to path\n",
    "    train(x_train,x_test,y_train,y_test,path)\n",
    "    print 'Done!'\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(BASE+'model/testmodel.pkl')\n",
    "net.eval()\n",
    "#x_t.shape is (X,17,60)\n",
    "result = F.softmax(net(x_t)).cpu().detach().numpy()\n",
    "y_pred = result.argmax(axis=1)\n",
    "print y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
